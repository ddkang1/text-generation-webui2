# by default the Dockerfile specifies these versions: 3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX
# however for me to work i had to specify the exact version for my card ( 2060 ) it was 7.5
# https://developer.nvidia.com/cuda-gpus you can find the version for your card here
TORCH_CUDA_ARCH_LIST=8.6

# these commands worked for me with roughly 4.5GB of vram
CLI_ARGS=--model wizardlm-30b-uncensored.ggmlv3.q6_K.bin --listen --n-gpu-layers 60 --extensions openai --n_ctx 4096 --threads 12 --mlock
#CLI_ARGS=--model WizardLM-30B-Uncensored.ggmlv3.q5_1.bin --listen --n-gpu-layers 45 --extensions openai --n_ctx 4096 --threads 8 --mlock
#CLI_ARGS= --extensions openai --listen --model WizardLM-30B-Uncensored --gpu-memory 20 20 20 15 --model_type llama --n_ctx 4096
#CLI_ARGS= --extensions openai --listen --model WizardLM-30B-Uncensored-GPTQ --autogptq --gpu-memory 20 20 20 20 --triton --n_ctx 4096
#CLI_ARGS= --extensions openai --listen --model guanaco-65B-GPTQ --wbits 4 --gpu-memory 8 12 13 --model_type llama --n_ctx 4096
#CLI_ARGS= --extensions openai --listen --model falcon-40b-instruct-GPTQ --gpu-memory 10 12 13 15 --autogptq --trust-remote-code --n_ctx 4096

# Output generated in 69.84 seconds (2.29 tokens/s, 160 tokens, context 1886, seed 33836009)
# Output generated in 77.38 seconds (2.07 tokens/s, 160 tokens, context 1886, seed 699250616)

# the following examples have been tested with the files linked in docs/README_docker.md:
# example running 13b with 4bit/128 groupsize        : CLI_ARGS=--model llama-13b-4bit-128g --wbits 4 --listen --groupsize 128 --pre_layer 25
# example with loading api extension and public share: CLI_ARGS=--model llama-7b-4bit --wbits 4 --listen --auto-devices --no-stream --extensions api --share
# example running 7b with 8bit groupsize             : CLI_ARGS=--model llama-7b --load-in-8bit --listen --auto-devices

# # the port the webui binds to on the host
# HOST_PORT=7860
# # the port the webui binds to inside the container
# CONTAINER_PORT=7860

# # the port the api binds to on the host
# HOST_API_PORT=5000
# # the port the api binds to inside the container
# CONTAINER_API_PORT=5000

# # the port the api stream endpoint binds to on the host
# HOST_API_STREAM_PORT=5005
# # the port the api stream endpoint binds to inside the container
# CONTAINER_API_STREAM_PORT=5005

# the version used to install text-generation-webui from
WEBUI_VERSION=HEAD

#OPENEDAI_EMBEDDING_MODEL=all-MiniLM-L6-v2
#OPENEDAI_PORT=8080
OPENEDAI_DEBUG=True